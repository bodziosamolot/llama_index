{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/examples/cookbooks/GraphRAG_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GraphRAG Implementation with LlamaIndex\n",
    "\n",
    "[GraphRAG (Graphs + Retrieval Augmented Generation)](https://www.microsoft.com/en-us/research/project/graphrag/) combines the strengths of Retrieval Augmented Generation (RAG) and Query-Focused Summarization (QFS) to effectively handle complex queries over large text datasets. While RAG excels in fetching precise information, it struggles with broader queries that require thematic understanding, a challenge that QFS addresses but cannot scale well. GraphRAG integrates these approaches to offer responsive and thorough querying capabilities across extensive, diverse text corpora.\n",
    "\n",
    "\n",
    "This notebook provides guidance on constructing the GraphRAG pipeline using the LlamaIndex PropertyGraph abstractions.\n",
    "\n",
    "\n",
    "**NOTE:** This is an approximate implementation of GraphRAG. We are currently developing a series of cookbooks that will detail the exact implementation of GraphRAG."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GraphRAG Aproach\n",
    "\n",
    "The GraphRAG involves two steps:\n",
    "\n",
    "1. Graph Generation - Creates Graph, builds communities and its summaries over the given document.\n",
    "2. Answer to the Query - Use summaries of the communities created from step-1 to answer the query.\n",
    "\n",
    "**Graph Generation:**\n",
    "\n",
    "1. **Source Documents to Text Chunks:** Source documents are divided into smaller text chunks for easier processing.\n",
    "\n",
    "2. **Text Chunks to Element Instances:** Each text chunk is analyzed to identify and extract entities and relationships, resulting in a list of tuples that represent these elements.\n",
    "\n",
    "3. **Element Instances to Element Summaries:** The extracted entities and relationships are summarized into descriptive text blocks for each element using the LLM.\n",
    "\n",
    "4. **Element Summaries to Graph Communities:** These entities, relationships and summaries form a graph, which is subsequently partitioned into communities using algorithms using Heirarchical Leiden to establish a hierarchical structure.\n",
    "\n",
    "5. **Graph Communities to Community Summaries:** The LLM generates summaries for each community, providing insights into the dataset’s overall topical structure and semantics.\n",
    "\n",
    "**Answering the Query:**\n",
    "\n",
    "**Community Summaries to Global Answers:** The summaries of the communities are utilized to respond to user queries. This involves generating intermediate answers, which are then consolidated into a comprehensive global answer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GraphRAG Pipeline Components\n",
    "\n",
    "Here are the different components we implemented to build all of the processes mentioned above.\n",
    "\n",
    "1. **Source Documents to Text Chunks:** Implemented using `SentenceSplitter` with a chunk size of 1024 and chunk overlap of 20 tokens.\n",
    "\n",
    "2. **Text Chunks to Element Instances AND Element Instances to Element Summaries:** Implemented using `GraphRAGExtractor`.\n",
    "\n",
    "3. **Element Summaries to Graph Communities AND Graph Communities to Community Summaries:** Implemented using `GraphRAGStore`.\n",
    "\n",
    "4. **Community Summaries to Global Answers:** Implemented using `GraphQueryEngine`.\n",
    "\n",
    "\n",
    "Let's check into each of these components and build GraphRAG pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "`graspologic` is used to use hierarchical_leiden for building communities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.10.0 environment at: /home/bodziosamolot/code/llama_index/.venv\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m133 packages\u001b[0m \u001b[2min 1.78s\u001b[0m\u001b[0m                                       \u001b[0m\n",
      "\u001b[2K\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/6)                                                   \n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/6)--------------\u001b[0m\u001b[0m     0 B/26.04 KiB           \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/6)2m------------\u001b[0m\u001b[0m 14.87 KiB/26.04 KiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/6)2m------------\u001b[0m\u001b[0m 14.87 KiB/26.04 KiB         \u001b[1A\n",
      "\u001b[2mflatbuffers         \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 14.87 KiB/26.04 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/6)--------------\u001b[0m\u001b[0m     0 B/60.15 KiB           \u001b[2A\n",
      "\u001b[2mflatbuffers         \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 14.87 KiB/26.04 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/6)--------------\u001b[0m\u001b[0m 14.88 KiB/60.15 KiB         \u001b[2A\n",
      "\u001b[2mflatbuffers         \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 26.04 KiB/26.04 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/6)--------------\u001b[0m\u001b[0m 14.88 KiB/60.15 KiB         \u001b[2A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/6)--------------\u001b[0m\u001b[0m 14.88 KiB/60.15 KiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/6)--------------\u001b[0m\u001b[0m 30.88 KiB/60.15 KiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/6)----\u001b[2m------\u001b[0m\u001b[0m 46.88 KiB/60.15 KiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/6)----\u001b[2m------\u001b[0m\u001b[0m 46.88 KiB/60.15 KiB         \u001b[1A\n",
      "\u001b[2mloguru              \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 46.88 KiB/60.15 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/6)--------------\u001b[0m\u001b[0m     0 B/315.88 KiB          \u001b[2A\n",
      "\u001b[2mloguru              \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 60.15 KiB/60.15 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/6)--------------\u001b[0m\u001b[0m     0 B/315.88 KiB          \u001b[2A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/6)--------------\u001b[0m\u001b[0m     0 B/315.88 KiB          \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/6)--------------\u001b[0m\u001b[0m 16.00 KiB/315.88 KiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/6)--------------\u001b[0m\u001b[0m 32.00 KiB/315.88 KiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/6)--------------\u001b[0m\u001b[0m 48.00 KiB/315.88 KiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/6)--------------\u001b[0m\u001b[0m 63.31 KiB/315.88 KiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/6)--------------\u001b[0m\u001b[0m 63.31 KiB/315.88 KiB        \u001b[1A\n",
      "\u001b[2mfastembed           \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/105.95 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/6)--------------\u001b[0m\u001b[0m 63.31 KiB/315.88 KiB        \u001b[2A\n",
      "\u001b[2mfastembed           \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/105.95 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/6)--------------\u001b[0m\u001b[0m 63.31 KiB/315.88 KiB        \u001b[2A\n",
      "\u001b[2mfastembed           \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 64.00 KiB/105.95 KiB\n",
      "\u001b[2mprotobuf            \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 127.31 KiB/315.88 KiB\n",
      "\u001b[2mpy-rust-stemmers    \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 26.71 KiB/317.18 KiB\n",
      "\u001b[2K\u001b[4A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/6)--------------\u001b[0m\u001b[0m 30.91 KiB/16.57 MiB         \u001b[4A\n",
      "\u001b[2mprotobuf            \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 171.11 KiB/315.88 KiB\n",
      "\u001b[2mpy-rust-stemmers    \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 52.22 KiB/317.18 KiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/6)--------------\u001b[0m\u001b[0m 110.91 KiB/16.57 MiB        \u001b[3A\n",
      "\u001b[2mprotobuf            \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 255.31 KiB/315.88 KiB\n",
      "\u001b[2mpy-rust-stemmers    \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 180.22 KiB/317.18 KiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/6)--------------\u001b[0m\u001b[0m 222.91 KiB/16.57 MiB        \u001b[3A\n",
      "\u001b[2mpy-rust-stemmers    \u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 276.22 KiB/317.18 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/6)--------------\u001b[0m\u001b[0m 318.91 KiB/16.57 MiB        \u001b[2A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/6)--------------\u001b[0m\u001b[0m 408.56 KiB/16.57 MiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/6)--------------\u001b[0m\u001b[0m 1.05 MiB/16.57 MiB          \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/6)--------------\u001b[0m\u001b[0m 3.17 MiB/16.57 MiB          \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/6)--------------\u001b[0m\u001b[0m 6.20 MiB/16.57 MiB          \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/6)2m------------\u001b[0m\u001b[0m 9.63 MiB/16.57 MiB          \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/6)----\u001b[2m------\u001b[0m\u001b[0m 12.94 MiB/16.57 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m6 packages\u001b[0m \u001b[2min 471ms\u001b[0m\u001b[0m                                                 \u001b[1A\n",
      "\u001b[2mUninstalled \u001b[1m1 package\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m10 packages\u001b[0m \u001b[2min 6ms\u001b[0m\u001b[0m                                \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcoloredlogs\u001b[0m\u001b[2m==15.0.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfastembed\u001b[0m\u001b[2m==0.7.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mflatbuffers\u001b[0m\u001b[2m==25.12.19\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhumanfriendly\u001b[0m\u001b[2m==10.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mloguru\u001b[0m\u001b[2m==0.7.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmmh3\u001b[0m\u001b[2m==5.2.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1monnxruntime\u001b[0m\u001b[2m==1.23.2\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mpillow\u001b[0m\u001b[2m==12.1.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpillow\u001b[0m\u001b[2m==11.3.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mprotobuf\u001b[0m\u001b[2m==6.33.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpy-rust-stemmers\u001b[0m\u001b[2m==0.1.5\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install llama-index graspologic numpy==1.24.4 scipy==1.12.0 llama-index-llms-groq llama-index-embeddings-fastembed fastembed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "We will use a sample news article dataset retrieved from Diffbot, which Tomaz has conveniently made available on GitHub for easy access.\n",
    "\n",
    "The dataset contains 2,500 samples; for ease of experimentation, we will use 50 of these samples, which include the `title` and `text` of news articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chevron: Best Of Breed</td>\n",
       "      <td>2031-04-06T01:36:32.000000000+00:00</td>\n",
       "      <td>JHVEPhoto Like many companies in the O&amp;G secto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FirstEnergy (NYSE:FE) Posts Earnings Results</td>\n",
       "      <td>2030-04-29T06:55:28.000000000+00:00</td>\n",
       "      <td>FirstEnergy (NYSE:FE – Get Rating) posted its ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dáil almost suspended after Sinn Féin TD put p...</td>\n",
       "      <td>2023-06-15T14:32:11.000000000+00:00</td>\n",
       "      <td>The Dáil was almost suspended on Thursday afte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Epic’s latest tool can animate hyperrealistic ...</td>\n",
       "      <td>2023-06-15T14:00:00.000000000+00:00</td>\n",
       "      <td>Today, Epic is releasing a new tool designed t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EU to Ban Huawei, ZTE from Internal Commission...</td>\n",
       "      <td>2023-06-15T13:50:00.000000000+00:00</td>\n",
       "      <td>The European Commission is planning to ban equ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                             Chevron: Best Of Breed   \n",
       "1       FirstEnergy (NYSE:FE) Posts Earnings Results   \n",
       "2  Dáil almost suspended after Sinn Féin TD put p...   \n",
       "3  Epic’s latest tool can animate hyperrealistic ...   \n",
       "4  EU to Ban Huawei, ZTE from Internal Commission...   \n",
       "\n",
       "                                  date  \\\n",
       "0  2031-04-06T01:36:32.000000000+00:00   \n",
       "1  2030-04-29T06:55:28.000000000+00:00   \n",
       "2  2023-06-15T14:32:11.000000000+00:00   \n",
       "3  2023-06-15T14:00:00.000000000+00:00   \n",
       "4  2023-06-15T13:50:00.000000000+00:00   \n",
       "\n",
       "                                                text  \n",
       "0  JHVEPhoto Like many companies in the O&G secto...  \n",
       "1  FirstEnergy (NYSE:FE – Get Rating) posted its ...  \n",
       "2  The Dáil was almost suspended on Thursday afte...  \n",
       "3  Today, Epic is releasing a new tool designed t...  \n",
       "4  The European Commission is planning to ban equ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from llama_index.core import Document\n",
    "\n",
    "news = pd.read_csv(\n",
    "    \"https://raw.githubusercontent.com/tomasonjo/blog-datasets/main/news_articles.csv\"\n",
    ")[:50]\n",
    "\n",
    "news.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare documents as required by LlamaIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_documents = [\n",
    "    Document(text=f\"{row['title']}: {row['text']}\")\n",
    "    for i, row in news.iterrows()\n",
    "]\n",
    "# Step is set to 4 to take every fourth document\n",
    "documents = all_documents[::4]\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimisation below did not help because it's the number of LLM calls which is the limiting factor, not their length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from colorama import Fore, Style\n",
    "# from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "# GRAY_LIGHT = \"\\033[38;5;250m\"\n",
    "# GRAY_DARK = \"\\033[38;5;240m\"\n",
    "# RESET = \"\\033[0m\"\n",
    "\n",
    "# splitter = SentenceSplitter(chunk_size=30, chunk_overlap=3)\n",
    "\n",
    "# documents = [\n",
    "#     Document(text=f\"{row['title']}: {row['text']}\")\n",
    "#     for i, row in news.iterrows()\n",
    "# ]\n",
    "\n",
    "# shortened_documents = []\n",
    "\n",
    "# for document in documents:\n",
    "#     print(Fore.GREEN + \"Original Document:\" + Style.RESET_ALL)\n",
    "#     print(document.text)\n",
    "#     print(Fore.BLUE + \"Split Nodes:\" + Style.RESET_ALL)\n",
    "#     nodes = splitter.get_nodes_from_documents([document])\n",
    "#     shortened_documents.append(nodes[0])\n",
    "#     for i, node in enumerate(nodes):\n",
    "#         shade = GRAY_LIGHT if i % 2 == 0 else GRAY_DARK\n",
    "#         print(f\"{shade}{node.text}{RESET}\")\n",
    "#         print()\n",
    "        \n",
    "# documents = shortened_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup API Key and LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bodziosamolot/code/llama_index/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "api_key = os.environ[\"GROQ_API_KEY\"]\n",
    "\n",
    "from llama_index.llms.groq import Groq\n",
    "\n",
    "llm = Groq(\n",
    "    model=\"qwen/qwen3-32b\",\n",
    "    api_key=api_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GraphRAGExtractor\n",
    "\n",
    "The GraphRAGExtractor class is designed to extract triples (subject-relation-object) from text and enrich them by adding descriptions for entities and relationships to their properties using an LLM.\n",
    "\n",
    "This functionality is similar to that of the `SimpleLLMPathExtractor`, but includes additional enhancements to handle entity, relationship descriptions. For guidance on implementation, you may look at similar existing [extractors](https://docs.llamaindex.ai/en/latest/examples/property_graph/dynamic_kg_extraction/?h=comparing).\n",
    "\n",
    "Here's a breakdown of its functionality:\n",
    "\n",
    "**Key Components:**\n",
    "\n",
    "1. `llm:` The language model used for extraction.\n",
    "2. `extract_prompt:` A prompt template used to guide the LLM in extracting information.\n",
    "3. `parse_fn:` A function to parse the LLM's output into structured data.\n",
    "4. `max_paths_per_chunk:` Limits the number of triples extracted per text chunk.\n",
    "5. `num_workers:` For parallel processing of multiple text nodes.\n",
    "\n",
    "\n",
    "**Main Methods:**\n",
    "\n",
    "1. `__call__:` The entry point for processing a list of text nodes.\n",
    "2. `acall:` An asynchronous version of __call__ for improved performance.\n",
    "3. `_aextract:` The core method that processes each individual node.\n",
    "\n",
    "\n",
    "**Extraction Process:**\n",
    "\n",
    "For each input node (chunk of text):\n",
    "1. It sends the text to the LLM along with the extraction prompt.\n",
    "2. The LLM's response is parsed to extract entities, relationships, descriptions for entities and relations.\n",
    "3. Entities are converted into EntityNode objects. Entity description is stored in metadata\n",
    "4. Relationships are converted into Relation objects. Relationship description is stored in metadata.\n",
    "5. These are added to the node's metadata under KG_NODES_KEY and KG_RELATIONS_KEY.\n",
    "\n",
    "**NOTE:** In the current implementation, we are using only relationship descriptions. In the next implementation, we will utilize entity descriptions during the retrieval stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from typing import Any, List, Callable, Optional, Union, Dict\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from llama_index.core.async_utils import run_jobs\n",
    "from llama_index.core.indices.property_graph.utils import (\n",
    "    default_parse_triplets_fn,\n",
    ")\n",
    "from llama_index.core.graph_stores.types import (\n",
    "    EntityNode,\n",
    "    KG_NODES_KEY,\n",
    "    KG_RELATIONS_KEY,\n",
    "    Relation,\n",
    ")\n",
    "from llama_index.core.llms.llm import LLM\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.core.prompts.default_prompts import (\n",
    "    DEFAULT_KG_TRIPLET_EXTRACT_PROMPT,\n",
    ")\n",
    "from llama_index.core.schema import TransformComponent, BaseNode\n",
    "from llama_index.core.bridge.pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class GraphRAGExtractor(TransformComponent):\n",
    "    \"\"\"Extract triples from a graph.\n",
    "\n",
    "    Uses an LLM and a simple prompt + output parsing to extract paths (i.e. triples) and entity, relation descriptions from text.\n",
    "\n",
    "    Args:\n",
    "        llm (LLM):\n",
    "            The language model to use.\n",
    "        extract_prompt (Union[str, PromptTemplate]):\n",
    "            The prompt to use for extracting triples.\n",
    "        parse_fn (callable):\n",
    "            A function to parse the output of the language model.\n",
    "        num_workers (int):\n",
    "            The number of workers to use for parallel processing.\n",
    "        max_paths_per_chunk (int):\n",
    "            The maximum number of paths to extract per chunk.\n",
    "    \"\"\"\n",
    "\n",
    "    llm: LLM\n",
    "    extract_prompt: PromptTemplate\n",
    "    parse_fn: Callable\n",
    "    num_workers: int\n",
    "    max_paths_per_chunk: int\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm: Optional[LLM] = None,\n",
    "        extract_prompt: Optional[Union[str, PromptTemplate]] = None,\n",
    "        parse_fn: Callable = default_parse_triplets_fn,\n",
    "        max_paths_per_chunk: int = 10,\n",
    "        num_workers: int = 4,\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "        from llama_index.core import Settings\n",
    "\n",
    "        if isinstance(extract_prompt, str):\n",
    "            extract_prompt = PromptTemplate(extract_prompt)\n",
    "\n",
    "        super().__init__(\n",
    "            llm=llm or Settings.llm,\n",
    "            extract_prompt=extract_prompt or DEFAULT_KG_TRIPLET_EXTRACT_PROMPT,\n",
    "            parse_fn=parse_fn,\n",
    "            num_workers=num_workers,\n",
    "            max_paths_per_chunk=max_paths_per_chunk,\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def class_name(cls) -> str:\n",
    "        return \"GraphExtractor\"\n",
    "\n",
    "    def __call__(\n",
    "        self, nodes: List[BaseNode], show_progress: bool = False, **kwargs: Any\n",
    "    ) -> List[BaseNode]:\n",
    "        \"\"\"Extract triples from nodes.\"\"\"\n",
    "        return asyncio.run(\n",
    "            self.acall(nodes, show_progress=show_progress, **kwargs)\n",
    "        )\n",
    "\n",
    "    async def _aextract(self, node: BaseNode) -> BaseNode:\n",
    "        \"\"\"Extract triples from a node.\"\"\"\n",
    "        assert hasattr(node, \"text\")\n",
    "\n",
    "        text = node.get_content(metadata_mode=\"llm\")\n",
    "        try:\n",
    "            await asyncio.sleep(1) # Spread the requests more to stay under the limit\n",
    "            llm_response = await self.llm.apredict(\n",
    "                self.extract_prompt,\n",
    "                text=text,\n",
    "                max_knowledge_triplets=self.max_paths_per_chunk,\n",
    "            )\n",
    "            entities, entities_relationship = self.parse_fn(llm_response)\n",
    "        except ValueError:\n",
    "            entities = []\n",
    "            entities_relationship = []\n",
    "\n",
    "        existing_nodes = node.metadata.pop(KG_NODES_KEY, [])\n",
    "        existing_relations = node.metadata.pop(KG_RELATIONS_KEY, [])\n",
    "        metadata = node.metadata.copy()\n",
    "        for entity, entity_type, description in entities:\n",
    "            metadata[\n",
    "                \"entity_description\"\n",
    "            ] = description  # Not used in the current implementation. But will be useful in future work.\n",
    "            entity_node = EntityNode(\n",
    "                name=entity, label=entity_type, properties=metadata\n",
    "            )\n",
    "            existing_nodes.append(entity_node)\n",
    "\n",
    "        metadata = node.metadata.copy()\n",
    "        for triple in entities_relationship:\n",
    "            subj, obj, rel, description = triple\n",
    "            subj_node = EntityNode(name=subj, properties=metadata)\n",
    "            obj_node = EntityNode(name=obj, properties=metadata)\n",
    "            metadata[\"relationship_description\"] = description\n",
    "            rel_node = Relation(\n",
    "                label=rel,\n",
    "                source_id=subj_node.id,\n",
    "                target_id=obj_node.id,\n",
    "                properties=metadata,\n",
    "            )\n",
    "\n",
    "            existing_nodes.extend([subj_node, obj_node])\n",
    "            existing_relations.append(rel_node)\n",
    "\n",
    "        node.metadata[KG_NODES_KEY] = existing_nodes\n",
    "        node.metadata[KG_RELATIONS_KEY] = existing_relations\n",
    "        return node\n",
    "\n",
    "    async def acall(\n",
    "        self, nodes: List[BaseNode], show_progress: bool = False, **kwargs: Any\n",
    "    ) -> List[BaseNode]:\n",
    "        \"\"\"Extract triples from nodes async.\"\"\"\n",
    "        jobs = []\n",
    "        for node in nodes:\n",
    "            jobs.append(self._aextract(node))\n",
    "\n",
    "        return await run_jobs(\n",
    "            jobs,\n",
    "            workers=self.num_workers,\n",
    "            show_progress=show_progress,\n",
    "            desc=\"Extracting paths from text\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TransformComponent\n",
    "\n",
    "It defines a contract for any component that transforms a sequence of nodes into another sequence of nodes. The interface is:\n",
    "\n",
    "__call__(nodes) — synchronous transform (abstract, must be implemented)\n",
    "acall(nodes) — async transform (default falls back to __call__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence type\n",
    "\n",
    "It's an abstract type representing any ordered, iterable collection that supports len() and indexing ([]), such as list, tuple, or range.\n",
    "\n",
    "Using Sequence instead of List in the base class is a common Python pattern — it makes the interface more flexible by not forcing callers to use a specific concrete type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GraphRAGStore\n",
    "\n",
    "The `GraphRAGStore` class is an extension of the `SimplePropertyGraphStore `class, designed to implement GraphRAG pipeline. Here's a breakdown of its key components and functions:\n",
    "\n",
    "\n",
    "The class uses community detection algorithms to group related nodes in the graph and then it generates summaries for each community using an LLM.\n",
    "\n",
    "\n",
    "**Key Methods:**\n",
    "\n",
    "`build_communities():`\n",
    "\n",
    "1. Converts the internal graph representation to a NetworkX graph. NetworkX graph - https://networkx.org/en/\n",
    "\n",
    "2. Applies the hierarchical Leiden algorithm for community detection.\n",
    "\n",
    "3. Collects detailed information about each community.\n",
    "\n",
    "4. Generates summaries for each community.\n",
    "\n",
    "`generate_community_summary(text):`\n",
    "\n",
    "1. Uses LLM to generate a summary of the relationships in a community.\n",
    "2. The summary includes entity names and a synthesis of relationship descriptions.\n",
    "\n",
    "`_create_nx_graph():`\n",
    "\n",
    "1. Converts the internal graph representation to a NetworkX graph for community detection.\n",
    "\n",
    "`_collect_community_info(nx_graph, clusters):`\n",
    "\n",
    "1. Collects detailed information about each node based on its community.\n",
    "2. Creates a string representation of each relationship within a community.\n",
    "\n",
    "`_summarize_communities(community_info):`\n",
    "\n",
    "1. Generates and stores summaries for each community using LLM.\n",
    "\n",
    "`get_community_summaries():`\n",
    "\n",
    "1. Returns the community summaries by building them if not already done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network X\n",
    "\n",
    "https://networkx.org/en/\n",
    "\n",
    "- Used for community detection\n",
    "- Contains:\n",
    "  - Nodes — each node from the internal self.graph.nodes is added as a string.\n",
    "  - Edges — each relation from self.graph.relations becomes an edge with two attributes:\n",
    "    - relationship: the relation label (e.g., \"works_at\", \"is_part_of\")\n",
    "    - description: a textual description of the relationship stored in relation.properties[\"relationship_description\"]\n",
    "- The sole purpose of this NetworkX graph is to feed it into the hierarchical Leiden algorithm (from the graspologic library) to detect communities — clusters of densely connected entities\n",
    "- Those communities are then summarized by an LLM, and the summaries power the GraphRAG query engine.\n",
    "- In short: it's an intermediate graph format that bridges LlamaIndex's internal property graph store with NetworkX's graph algorithms for community detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Communities\n",
    "\n",
    "A community in this GraphRAG context is a cluster of closely related entities in the knowledge graph — a group of nodes that are more densely connected to each other than to the rest of the graph.\n",
    "\n",
    "Here's how they're constructed from triplets, step by step:\n",
    "\n",
    "1. Triplet extraction\n",
    "The SchemaLLMPathExtractor processes each text chunk and extracts triplets of the form:\n",
    "\n",
    "(entity1) → [relationship] → (entity2)\n",
    "\n",
    "For example, from a news article you might get:\n",
    "\n",
    "(\"Tesla\", \"manufactures\", \"Model 3\")\n",
    "(\"Elon Musk\", \"is CEO of\", \"Tesla\")\n",
    "(\"Tesla\", \"headquartered in\", \"Austin\")\n",
    "\n",
    "These triplets are stored in the internal property graph as nodes and relations.\n",
    "\n",
    "2. Conversion to a NetworkX graph\n",
    "The _create_nx_graph() method converts all those triplets into an undirected NetworkX graph:\n",
    "\n",
    "Each entity becomes a node\n",
    "Each relationship between two entities becomes an edge (with the relationship label and description as attributes)\n",
    "So the triplets above would produce nodes {Tesla, Model 3, Elon Musk, Austin} connected by edges.\n",
    "\n",
    "3. Community detection via hierarchical Leiden\n",
    "The Leiden algorithm (from the graspologic library) is run on this NetworkX graph. It optimizes modularity — a measure of how well nodes split into groups where:\n",
    "\n",
    "Nodes within a community have many connections to each other\n",
    "Nodes across communities have few connections\n",
    "The hierarchical variant produces nested communities at multiple levels of granularity, controlled by max_cluster_size.\n",
    "\n",
    "In the example above, Tesla, Model 3, Elon Musk, and Austin would likely land in the same community because they're all directly connected through Tesla.\n",
    "\n",
    "4. Community info collection\n",
    "_collect_community_info() iterates over each community and collects all the intra-community edges — i.e., relationships where both endpoints belong to the same cluster. It formats them as:\n",
    "\n",
    "entity1 -> entity2 -> relationship_label -> relationship_description\n",
    "\n",
    "5. Community summarization\n",
    "Finally, _summarize_communities() sends each community's collected relationship strings to an LLM, which produces a natural-language summary capturing the key entities and their relationships within that cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bodziosamolot/code/llama_index/.venv/lib/python3.10/site-packages/graspologic/layouts/colors.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from typing import Optional\n",
    "from llama_index.core.graph_stores import SimplePropertyGraphStore\n",
    "from llama_index.core.llms.llm import LLM\n",
    "import networkx as nx\n",
    "from graspologic.partition import hierarchical_leiden\n",
    "\n",
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "\n",
    "class GraphRAGStore(SimplePropertyGraphStore):\n",
    "    community_summary: dict = {}\n",
    "    max_cluster_size: int = 5\n",
    "    llm: Optional[LLM] = None\n",
    "\n",
    "    def __init__(self, llm: Optional[LLM] = None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.llm = llm\n",
    "\n",
    "    def generate_community_summary(self, text):\n",
    "        \"\"\"Generate summary for a given text using an LLM.\"\"\"\n",
    "        messages = [\n",
    "            ChatMessage(\n",
    "                role=\"system\",\n",
    "                content=(\n",
    "                    \"You are provided with a set of relationships from a knowledge graph, each represented as \"\n",
    "                    \"entity1->entity2->relation->relationship_description. Your task is to create a summary of these \"\n",
    "                    \"relationships. The summary should include the names of the entities involved and a concise synthesis \"\n",
    "                    \"of the relationship descriptions. The goal is to capture the most critical and relevant details that \"\n",
    "                    \"highlight the nature and significance of each relationship. Ensure that the summary is coherent and \"\n",
    "                    \"integrates the information in a way that emphasizes the key aspects of the relationships.\"\n",
    "                ),\n",
    "            ),\n",
    "            ChatMessage(role=\"user\", content=text),\n",
    "        ]\n",
    "        response = self.llm.chat(messages)\n",
    "        clean_response = re.sub(r\"^assistant:\\s*\", \"\", str(response)).strip()\n",
    "        return clean_response\n",
    "\n",
    "    def build_communities(self):\n",
    "        \"\"\"Builds communities from the graph and summarizes them.\"\"\"\n",
    "        nx_graph = self._create_nx_graph()\n",
    "        community_hierarchical_clusters = hierarchical_leiden(\n",
    "            nx_graph, max_cluster_size=self.max_cluster_size\n",
    "        )\n",
    "        community_info = self._collect_community_info(\n",
    "            nx_graph, community_hierarchical_clusters\n",
    "        )\n",
    "        self._summarize_communities(community_info)\n",
    "\n",
    "    def _create_nx_graph(self):\n",
    "        \"\"\"Converts internal graph representation to NetworkX graph.\"\"\"\n",
    "        nx_graph = nx.Graph()\n",
    "        for node in self.graph.nodes.values():\n",
    "            nx_graph.add_node(str(node))\n",
    "        for relation in self.graph.relations.values():\n",
    "            nx_graph.add_edge(\n",
    "                relation.source_id,\n",
    "                relation.target_id,\n",
    "                relationship=relation.label,\n",
    "                description=relation.properties[\"relationship_description\"],\n",
    "            )\n",
    "        return nx_graph\n",
    "\n",
    "    def _collect_community_info(self, nx_graph, clusters):\n",
    "        \"\"\"Collect detailed information for each node based on their community.\"\"\"\n",
    "        community_mapping = {item.node: item.cluster for item in clusters}\n",
    "        community_info = {}\n",
    "        for item in clusters:\n",
    "            cluster_id = item.cluster\n",
    "            node = item.node\n",
    "            if cluster_id not in community_info:\n",
    "                community_info[cluster_id] = []\n",
    "\n",
    "            for neighbor in nx_graph.neighbors(node):\n",
    "                if community_mapping[neighbor] == cluster_id:\n",
    "                    edge_data = nx_graph.get_edge_data(node, neighbor)\n",
    "                    if edge_data:\n",
    "                        detail = f\"{node} -> {neighbor} -> {edge_data['relationship']} -> {edge_data['description']}\"\n",
    "                        community_info[cluster_id].append(detail)\n",
    "        return community_info\n",
    "\n",
    "    def _summarize_communities(self, community_info):\n",
    "        \"\"\"Generate and store summaries for each community.\"\"\"\n",
    "        for community_id, details in community_info.items():\n",
    "            details_text = (\n",
    "                \"\\n\".join(details) + \".\"\n",
    "            )  # Ensure it ends with a period\n",
    "            self.community_summary[\n",
    "                community_id\n",
    "            ] = self.generate_community_summary(details_text)\n",
    "\n",
    "    def get_community_summaries(self):\n",
    "        \"\"\"Returns the community summaries, building them if not already done.\"\"\"\n",
    "        if not self.community_summary:\n",
    "            self.build_communities()\n",
    "        return self.community_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SimplePropertyGraphStore\n",
    "\n",
    "It's an in-memory property graph store that inherits from the abstract PropertyGraphStore\n",
    "\n",
    "Key characteristics:\n",
    "\n",
    "- Storage: Holds a LabelledPropertyGraph object in memory (self.graph) containing nodes, relations, and triplets.\n",
    "- No external database — everything lives in Python dictionaries. It can be persisted to/loaded from JSON files.\n",
    "- Does NOT support structured queries (e.g., Cypher) or vector queries — those raise NotImplementedError. This distinguishes it from database-backed stores like Neo4j."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GraphRAGQueryEngine\n",
    "\n",
    "The GraphRAGQueryEngine class is a custom query engine designed to process queries using the GraphRAG approach. It leverages the community summaries generated by the GraphRAGStore to answer user queries. Here's a breakdown of its functionality:\n",
    "\n",
    "**Main Components:**\n",
    "\n",
    "`graph_store:` An instance of GraphRAGStore, which contains the community summaries.\n",
    "`llm:` A Language Model (LLM) used for generating and aggregating answers.\n",
    "\n",
    "\n",
    "**Key Methods:**\n",
    "\n",
    "`custom_query(query_str: str)`\n",
    "\n",
    "1. This is the main entry point for processing a query. It retrieves community summaries, generates answers from each summary, and then aggregates these answers into a final response.\n",
    "\n",
    "`generate_answer_from_summary(community_summary, query):`\n",
    "\n",
    "1. Generates an answer for the query based on a single community summary.\n",
    "Uses the LLM to interpret the community summary in the context of the query.\n",
    "\n",
    "`aggregate_answers(community_answers):`\n",
    "\n",
    "1. Combines individual answers from different communities into a coherent final response.\n",
    "2. Uses the LLM to synthesize multiple perspectives into a single, concise answer.\n",
    "\n",
    "\n",
    "**Query Processing Flow:**\n",
    "\n",
    "1. Retrieve community summaries from the graph store.\n",
    "2. For each community summary, generate a specific answer to the query.\n",
    "3. Aggregate all community-specific answers into a final, coherent response.\n",
    "\n",
    "\n",
    "**Example usage:**\n",
    "\n",
    "```\n",
    "query_engine = GraphRAGQueryEngine(graph_store=graph_store, llm=llm)\n",
    "\n",
    "response = query_engine.query(\"query\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import CustomQueryEngine\n",
    "from llama_index.core.llms import LLM\n",
    "\n",
    "\n",
    "class GraphRAGQueryEngine(CustomQueryEngine):\n",
    "    graph_store: GraphRAGStore\n",
    "    llm: LLM\n",
    "\n",
    "    def custom_query(self, query_str: str) -> str:\n",
    "        \"\"\"Process all community summaries to generate answers to a specific query.\"\"\"\n",
    "        community_summaries = self.graph_store.get_community_summaries()\n",
    "        community_answers = [\n",
    "            self.generate_answer_from_summary(community_summary, query_str)\n",
    "            for _, community_summary in community_summaries.items()\n",
    "        ]\n",
    "\n",
    "        final_answer = self.aggregate_answers(community_answers)\n",
    "        return final_answer\n",
    "\n",
    "    def generate_answer_from_summary(self, community_summary, query):\n",
    "        \"\"\"Generate an answer from a community summary based on a given query using LLM.\"\"\"\n",
    "        prompt = (\n",
    "            f\"Given the community summary: {community_summary}, \"\n",
    "            f\"how would you answer the following query? Query: {query}\"\n",
    "        )\n",
    "        messages = [\n",
    "            ChatMessage(role=\"system\", content=prompt),\n",
    "            ChatMessage(\n",
    "                role=\"user\",\n",
    "                content=\"I need an answer based on the above information.\",\n",
    "            ),\n",
    "        ]\n",
    "        response = self.llm.chat(messages)\n",
    "        cleaned_response = re.sub(r\"^assistant:\\s*\", \"\", str(response)).strip()\n",
    "        return cleaned_response\n",
    "\n",
    "    def aggregate_answers(self, community_answers):\n",
    "        \"\"\"Aggregate individual community answers into a final, coherent response.\"\"\"\n",
    "        # intermediate_text = \" \".join(community_answers)\n",
    "        prompt = \"Combine the following intermediate answers into a final, concise response.\"\n",
    "        messages = [\n",
    "            ChatMessage(role=\"system\", content=prompt),\n",
    "            ChatMessage(\n",
    "                role=\"user\",\n",
    "                content=f\"Intermediate answers: {community_answers}\",\n",
    "            ),\n",
    "        ]\n",
    "        final_response = self.llm.chat(messages)\n",
    "        cleaned_final_response = re.sub(\n",
    "            r\"^assistant:\\s*\", \"\", str(final_response)\n",
    "        ).strip()\n",
    "        return cleaned_final_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Build End to End GraphRAG Pipeline\n",
    "\n",
    "Now that we have defined all the necessary components, let’s construct the GraphRAG pipeline:\n",
    "\n",
    "1. Create nodes/chunks from the text.\n",
    "2. Build a PropertyGraphIndex using `GraphRAGExtractor` and `GraphRAGStore`.\n",
    "3. Construct communities and generate a summary for each community using the graph built above.\n",
    "4. Create a `GraphRAGQueryEngine` and begin querying."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create nodes/ chunks from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "splitter = SentenceSplitter(\n",
    "    chunk_size=1024,\n",
    "    chunk_overlap=20,\n",
    ")\n",
    "nodes = splitter.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build ProperGraphIndex using `GraphRAGExtractor` and `GraphRAGStore`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "KG_TRIPLET_EXTRACT_TMPL = \"\"\"\n",
    "-Goal-\n",
    "Given a text document, identify all entities and their entity types from the text and all relationships among the identified entities.\n",
    "Given the text, extract up to {max_knowledge_triplets} entity-relation triplets.\n",
    "\n",
    "-Steps-\n",
    "1. Identify all entities. For each identified entity, extract the following information:\n",
    "- entity_name: Name of the entity, capitalized\n",
    "- entity_type: Type of the entity\n",
    "- entity_description: Comprehensive description of the entity's attributes and activities\n",
    "\n",
    "2. From the entities identified in step 1, identify all pairs of (source_entity, target_entity) that are *clearly related* to each other.\n",
    "For each pair of related entities, extract the following information:\n",
    "- source_entity: name of the source entity, as identified in step 1\n",
    "- target_entity: name of the target entity, as identified in step 1\n",
    "- relation: relationship between source_entity and target_entity\n",
    "- relationship_description: explanation as to why you think the source entity and the target entity are related to each other\n",
    "\n",
    "3. Output Formatting:\n",
    "- Return the result in valid JSON format with two keys: 'entities' (list of entity objects) and 'relationships' (list of relationship objects).\n",
    "- Exclude any text outside the JSON structure (e.g., no explanations or comments).\n",
    "- If no entities or relationships are identified, return empty lists: { \"entities\": [], \"relationships\": [] }.\n",
    "\n",
    "-An Output Example-\n",
    "{\n",
    "  \"entities\": [\n",
    "    {\n",
    "      \"entity_name\": \"Albert Einstein\",\n",
    "      \"entity_type\": \"Person\",\n",
    "      \"entity_description\": \"Albert Einstein was a theoretical physicist who developed the theory of relativity and made significant contributions to physics.\"\n",
    "    },\n",
    "    {\n",
    "      \"entity_name\": \"Theory of Relativity\",\n",
    "      \"entity_type\": \"Scientific Theory\",\n",
    "      \"entity_description\": \"A scientific theory developed by Albert Einstein, describing the laws of physics in relation to observers in different frames of reference.\"\n",
    "    },\n",
    "    {\n",
    "      \"entity_name\": \"Nobel Prize in Physics\",\n",
    "      \"entity_type\": \"Award\",\n",
    "      \"entity_description\": \"A prestigious international award in the field of physics, awarded annually by the Royal Swedish Academy of Sciences.\"\n",
    "    }\n",
    "  ],\n",
    "  \"relationships\": [\n",
    "    {\n",
    "      \"source_entity\": \"Albert Einstein\",\n",
    "      \"target_entity\": \"Theory of Relativity\",\n",
    "      \"relation\": \"developed\",\n",
    "      \"relationship_description\": \"Albert Einstein is the developer of the theory of relativity.\"\n",
    "    },\n",
    "    {\n",
    "      \"source_entity\": \"Albert Einstein\",\n",
    "      \"target_entity\": \"Nobel Prize in Physics\",\n",
    "      \"relation\": \"won\",\n",
    "      \"relationship_description\": \"Albert Einstein won the Nobel Prize in Physics in 1921.\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "-Real Data-\n",
    "######################\n",
    "text: {text}\n",
    "######################\n",
    "output:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def parse_fn(response_str: str) -> Any:\n",
    "    json_pattern = r\"\\{.*\\}\"\n",
    "    match = re.search(json_pattern, response_str, re.DOTALL)\n",
    "    entities = []\n",
    "    relationships = []\n",
    "    if not match:\n",
    "        return entities, relationships\n",
    "    json_str = match.group(0)\n",
    "    try:\n",
    "        data = json.loads(json_str)\n",
    "        entities = [\n",
    "            (\n",
    "                entity[\"entity_name\"],\n",
    "                entity[\"entity_type\"],\n",
    "                entity[\"entity_description\"],\n",
    "            )\n",
    "            for entity in data.get(\"entities\", [])\n",
    "        ]\n",
    "        relationships = [\n",
    "            (\n",
    "                relation[\"source_entity\"],\n",
    "                relation[\"target_entity\"],\n",
    "                relation[\"relation\"],\n",
    "                relation[\"relationship_description\"],\n",
    "            )\n",
    "            for relation in data.get(\"relationships\", [])\n",
    "        ]\n",
    "        return entities, relationships\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(\"Error parsing JSON:\", e)\n",
    "        return entities, relationships\n",
    "\n",
    "\n",
    "kg_extractor = GraphRAGExtractor(\n",
    "    llm=llm,\n",
    "    extract_prompt=KG_TRIPLET_EXTRACT_TMPL,\n",
    "    max_paths_per_chunk=2,\n",
    "    parse_fn=parse_fn,\n",
    "    num_workers=1, # Increasing the number of workers would risk exceeding the Requests Per Minute rate limit\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded graph store from cache.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from llama_index.core import PropertyGraphIndex\n",
    "\n",
    "GRAPH_STORE_PATH = \"graph_store.json\"\n",
    "COMMUNITY_SUMMARIES_PATH = \"community_summaries.json\"\n",
    "\n",
    "if os.path.exists(GRAPH_STORE_PATH):\n",
    "    base_store = SimplePropertyGraphStore.from_persist_path(GRAPH_STORE_PATH)\n",
    "    graph_store = GraphRAGStore(llm=llm)\n",
    "    graph_store.graph = base_store.graph\n",
    "    index = PropertyGraphIndex.from_existing(\n",
    "        property_graph_store=graph_store,\n",
    "        embed_kg_nodes=False,\n",
    "        llm=llm,\n",
    "    )\n",
    "    print(\"Loaded graph store from cache.\")\n",
    "else:\n",
    "    index = PropertyGraphIndex(\n",
    "        nodes=nodes,\n",
    "        llm=llm,\n",
    "        property_graph_store=GraphRAGStore(llm=llm),\n",
    "        kg_extractors=[kg_extractor],\n",
    "        embed_kg_nodes=False,\n",
    "        show_progress=True,\n",
    "    )\n",
    "    index.property_graph_store.persist(GRAPH_STORE_PATH)\n",
    "    print(f\"Graph store built and persisted to {GRAPH_STORE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:  Israel\n",
      "Label: entity\n",
      "Properties:\n",
      "  relationship_description: Uber announced the closure of its food delivery operations in Italy due to insufficient market share and competition from local providers.\n",
      "  triplet_source_id: c70953e4-b065-48d6-99a6-b72a58baa5ef\n"
     ]
    }
   ],
   "source": [
    "node = list(index.property_graph_store.graph.nodes.values())[-1]\n",
    "print(f\"Name:  {node.name}\")\n",
    "print(f\"Label: {node.label}\")\n",
    "print(f\"Properties:\")\n",
    "for k, v in node.properties.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Relation(label='operates in', source_id='Chevron', target_id='O&G Sector', properties={'relationship_description': 'Chevron is a company that operates within the oil and gas (O&G) industry sector.', 'triplet_source_id': '58897347-bc34-4697-abb2-f65d1e4dc272'})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(index.property_graph_store.graph.relations.values())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chevron is a company that operates within the oil and gas (O&G) industry sector.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(index.property_graph_store.graph.relations.values())[0].properties[\n",
    "    \"relationship_description\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build communities\n",
    "\n",
    "Creates communities and a summary for each community. Loads from cache if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded community summaries from cache.\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(COMMUNITY_SUMMARIES_PATH):\n",
    "    with open(COMMUNITY_SUMMARIES_PATH) as f:\n",
    "        index.property_graph_store.community_summary = {int(k): v for k, v in json.load(f).items()}\n",
    "    print(\"Loaded community summaries from cache.\")\n",
    "else:\n",
    "    index.property_graph_store.build_communities()\n",
    "    with open(COMMUNITY_SUMMARIES_PATH, \"w\") as f:\n",
    "        json.dump(index.property_graph_store.community_summary, f)\n",
    "    print(f\"Community summaries built and persisted to {COMMUNITY_SUMMARIES_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retries on Rate Limitted responses\n",
    "\n",
    "GraphRAGQueryEngine does get automatic retries on 429 responses — but the retry logic lives in the Groq LLM integration, not in the query engine itself.\n",
    "\n",
    "The GraphRAGQueryEngine itself has no retry logic.\n",
    "\n",
    "Retries use the following library: https://tenacity.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create QueryEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = GraphRAGQueryEngine(\n",
    "    graph_store=index.property_graph_store, llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<think>\n",
       "Okay, let's tackle this. The user wants the main news from the document, but looking at the provided info, there's no actual document. The intermediate answers mention various topics like Chevron's industry, European Commission's ban on Huawei and ZTE, Vivo's X90s, Apple's ecosystem, Square Enix remastering games, CarMax entities, JetBlue's livery, Coinbase's note repurchase, Allegiant Travel's stock upgrade, Uber's market exits, Gordon McQueen's dementia, and Manchester United's transfer interest.\n",
       "\n",
       "Wait, the user might have intended to provide a document but forgot. The initial answers correctly point out that there's no document, just knowledge graph data. But some answers assume there's a document and list news points. The user's query is a bit conflicting. I need to reconcile these.\n",
       "\n",
       "The correct approach is to clarify that there's no actual document, but if we consider the knowledge graph as the source, the main points are the relationships mentioned. However, some answers treated the knowledge graph as a document and extracted news-like points. The user might be confused between the two.\n",
       "\n",
       "I should start by stating that there's no document, but if we proceed with the knowledge graph, the main topics are the entities and their relationships. However, some answers listed specific news items based on the data. To resolve this, the final answer should first clarify the absence of a document, then summarize the key points from the knowledge graph as if they were the main topics discussed. Need to mention that these are structured relationships, not news articles. Also, ensure the answer is concise and addresses the user's possible confusion.\n",
       "</think>\n",
       "\n",
       "The provided information does not include a document or news article but consists of structured relationships from a knowledge graph. However, based on the data, the **main topics covered** are:  \n",
       "\n",
       "1. **Corporate/Financial Relationships**:  \n",
       "   - Entities like Chevron (O&G sector, NYSE listing), CarMax Auto Owner Trust (sponsorship, servicing), and Coinbase (convertible notes repurchase) highlight operational and financial frameworks.  \n",
       "\n",
       "2. **Technology & Market Moves**:  \n",
       "   - **Huawei/ZTE Ban**: European Commission’s plan to exclude high-risk vendors from internal networks.  \n",
       "   - **Vivo X90s**: Teaser by Jia Jingdong and confirmation of MediaTek Dimensity 9200+ SoC.  \n",
       "   - **Square Enix**: Remastering *Star Ocean* games (*First Departure R*, *The Second Story R*).  \n",
       "\n",
       "3. **Corporate Strategy & Leadership**:  \n",
       "   - **Apple**: Ecosystem integration (iCloud, Apple Pay), partnerships (Intel, TSMC), and competition with Microsoft.  \n",
       "   - **JetBlue**: Fleet upgrades (Mint Suites, New Standard Livery) for premium travel.  \n",
       "\n",
       "4. **Market Exits & Acquisitions**:  \n",
       "   - **Uber**: Exited food delivery in Italy and taxi services in Israel due to competition.  \n",
       "   - **Allegiant Travel**: Deutsche Bank upgraded its stock amid improving ROIC.  \n",
       "\n",
       "5. **Sports & Entertainment**:  \n",
       "   - **Manchester City**: Potential Champions League matchups with Real Madrid and Inter Milan.  \n",
       "   - **Gordon McQueen**: Diagnosed with vascular dementia in 2021.  \n",
       "\n",
       "These points reflect structured data rather than news articles. If you intended to reference a specific document, please provide it for a precise summary."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = query_engine.query(\n",
    "    \"What are the main news discussed in the document?\"\n",
    ")\n",
    "display(Markdown(f\"{response.response}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<think>\n",
       "Okay, let's tackle this. The user wants a concise final response combining all the intermediate answers. First, I need to look at each intermediate answer to see what they have in common.\n",
       "\n",
       "Most of the intermediate answers start by stating that the provided information doesn't include specific financial sector news. They then mention the actual topic covered in the data, like Chevron's stock, EU telecom policies, Vivo phones, sports contracts, etc. Each one explains that the context is unrelated to finance and suggests checking other sources for real financial news.\n",
       "\n",
       "I need to make sure the final answer is concise but covers all the key points. The main idea is that the user asked for financial news, but the available data is about other topics. So the response should clarify that there's no financial news here and list the actual topics covered in the data. Also, it should offer to help with other queries if needed.\n",
       "\n",
       "I should avoid repeating the same phrases from each intermediate answer. Instead, summarize the common elements: no financial news, mention the various unrelated topics, and suggest consulting other sources. Make sure it's clear and to the point without being too verbose.\n",
       "</think>\n",
       "\n",
       "**Final Answer:**  \n",
       "The provided information does not include specific news related to the financial sector. The content spans unrelated topics such as Chevron’s NYSE listing, EU telecom policies, smartphone technology, sports contracts, and corporate financial structures (e.g., CarMax securitization). For financial sector updates, consult dedicated financial news sources or provide additional context for targeted assistance."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = query_engine.query(\"What are news related to financial sector?\")\n",
    "display(Markdown(f\"{response.response}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Work:\n",
    "\n",
    "This cookbook is an approximate implementation of GraphRAG. In future cookbooks, we plan to extend it as follows:\n",
    "\n",
    "1. Implement retrieval using entity description embeddings.\n",
    "2. Integrate with Neo4JPropertyGraphStore.\n",
    "3. Calculate a helpfulness score for each answer generated from the community summaries and filter out answers where the helpfulness score is zero.\n",
    "4. Perform entity disambiguation to remove duplicate entities.\n",
    "5. Implement claims or covariate information extraction, Local Search and Global Search techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison: GraphRAG vs. Regular RAG\n",
    "\n",
    "Let's build a standard vector-based RAG pipeline using the same documents and LLM, then compare the answers side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 5 files: 100%|██████████| 5/5 [00:01<00:00,  2.76it/s]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core import Settings\n",
    "from llama_index.embeddings.fastembed import FastEmbedEmbedding\n",
    "\n",
    "Settings.embed_model = FastEmbedEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "vector_index = VectorStoreIndex(nodes=nodes)\n",
    "vector_query_engine = vector_index.as_query_engine(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Query: What are the main news discussed in the document?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**GraphRAG:**\n",
       "\n",
       "<think>\n",
       "Okay, let's see. The user is asking for the main news discussed in the document based on the provided information. The intermediate answers given are all different responses to similar queries but with varying contexts. Each one addresses a different topic, like Chevron's industry, European Commission's ban on Huawei and ZTE, Vivo's new phone, Apple's strategies, Square Enix's remastered games, etc.\n",
       "\n",
       "Wait, the user's actual query is about combining these intermediate answers into a final, concise response. But looking at the history, each intermediate answer is a response to a different document or context. However, the user now wants to combine all these into one final answer. But that doesn't make sense because each intermediate answer is for a different topic. Maybe there's a misunderstanding here. \n",
       "\n",
       "Wait, perhaps the user is referring to a specific document that was mentioned in the initial query, but the assistant provided multiple intermediate answers for different documents. Now, the user wants to combine all those answers into one. But the problem is that each intermediate answer is for a different topic. So combining them would result in a list of unrelated news items. \n",
       "\n",
       "Alternatively, maybe the user is asking to create a single answer that addresses all the different topics covered in the intermediate answers. But that's not typical. Usually, each query is separate. However, given the way the question is phrased, the user might have intended to ask for a summary of all the different news items mentioned across various documents. \n",
       "\n",
       "Looking at the intermediate answers, each one is a separate news summary. For example, one is about Chevron, another about European Commission's ban, another about Vivo's phone, etc. So the final answer should list all these as separate news items. But the user wants a concise response. So perhaps the correct approach is to list each main news item from each intermediate answer, but that would be a long list. However, the user specified \"concise,\" so maybe just a brief mention of each topic without going into details. \n",
       "\n",
       "But the user's instruction says \"Combine the following intermediate answers into a final, concise response.\" So the assistant needs to take all the intermediate answers (each being a separate news summary) and combine them into one response. However, since they are all different topics, the final answer would be a list of each main news item from each intermediate answer. \n",
       "\n",
       "But the user might have intended that each intermediate answer is part of the same document, but that's not indicated. Given the ambiguity, the safest approach is to list each main news item from each intermediate answer as separate points in the final response. However, since the user wants it concise, maybe grouping similar topics or summarizing each briefly. \n",
       "\n",
       "Wait, looking at the intermediate answers, each one is a separate answer to a different query. For example, one is about Chevron, another about European Commission, another about Vivo, etc. So combining them would mean listing all these as separate news items. But the user might not want that. Alternatively, maybe the user made a mistake in providing multiple intermediate answers for different queries and now wants a single answer that addresses all. However, without more context, it's challenging. \n",
       "\n",
       "Given the instructions, the assistant should proceed to combine all the intermediate answers into a single, concise response. The best way is to list each main news item from each intermediate answer as bullet points or numbered items. However, since the user wants it concise, maybe just a brief summary of each. \n",
       "\n",
       "So the final answer would be a list of each main news item from each intermediate answer, presented concisely. For example:\n",
       "\n",
       "1. Chevron's operations in the O&G sector and NYSE listing.\n",
       "2. European Commission's ban on Huawei and ZTE.\n",
       "3. Jia Jingdong's teaser for Vivo X90s with MediaTek SoC.\n",
       "4. Apple's product lines, leadership, and partnerships.\n",
       "5. Square Enix's remastered Star Ocean games.\n",
       "6. CarMax's trust relationships.\n",
       "7. JetBlue's New Standard Livery.\n",
       "8. Coinbase's convertible notes repurchase.\n",
       "9. Allegiant Travel's stock upgrade and ROIC.\n",
       "10. Manchester City's potential Champions League matchups.\n",
       "11. Uber's exits from Italy and Israel.\n",
       "12. Gordon McQueen's dementia diagnosis.\n",
       "13. Manchester United and Chelsea's Onana transfer race.\n",
       "\n",
       "But the user wants it concise, so maybe grouping similar topics or summarizing each briefly. However, given the diversity of topics, it's best to list each as a separate point. The assistant should present each main news item from the intermediate answers in a clear, concise manner, ensuring that each is distinct and summarized effectively.\n",
       "</think>\n",
       "\n",
       "The main news discussed across the provided documents includes:  \n",
       "\n",
       "1. **Chevron**: Operates in the oil and gas sector, listed on NYSE under ticker CVX.  \n",
       "2. **European Commission**: Plans to ban Huawei and ZTE from internal networks due to security concerns, aligning with broader high-risk vendor exclusion strategies.  \n",
       "3. **Vivo X90s**: Teased by Jia Jingdong, expected to feature MediaTek Dimensity 9200+ SoC, highlighting design and performance upgrades.  \n",
       "4. **Apple**: Focus on product innovation (iPhone, MacBooks), leadership under Tim Cook, partnerships (Intel, TSMC), and ecosystem expansion (Apple Music, iCloud).  \n",
       "5. **Square Enix**: Remastering *Star Ocean: First Departure R* (PS4/Switch) and potential *Star Ocean: The Second Story R* (leaked evidence).  \n",
       "6. **CarMax Trust**: Structural roles of CarMax Business Services, LLC (sponsor/servicer) and CarMax Auto Funding LLC (depositor).  \n",
       "7. **JetBlue**: Launch of New Standard Livery for aircraft, reflecting brand evolution and strategic innovation.  \n",
       "8. **Coinbase**: Repurchased $64.5M convertible notes at 29% discount in 2023, managed under CFO Alesia Haas’s financial strategy.  \n",
       "9. **Allegiant Travel**: Deutsche Bank upgraded its stock to \"Buy\" amid improving Return on Invested Capital (ROIC).  \n",
       "10. **UEFA Champions League**: Potential 2023/24 knockout stage matchups between Manchester City, Real Madrid, and Inter Milan.  \n",
       "11. **Uber**: Exited food delivery in Italy and taxi/private hire markets in Israel due to competition and operational challenges.  \n",
       "12. **Gordon McQueen**: Diagnosed with vascular dementia in 2021, which he battled until his death.  \n",
       "13. **Transfer News**: Manchester United and Chelsea competing for Inter Milan’s goalkeeper André Onana, with United seeking a de Gea replacement.  \n",
       "\n",
       "Each point reflects key updates or strategic developments from the respective domains."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Regular RAG:**\n",
       "\n",
       "<think>\n",
       "Okay, let's tackle this query. The user is asking for the main news discussed in the document provided. First, I need to parse through the context information given.\n",
       "\n",
       "Looking at the first part, there's a press release from Coinbase about repurchasing some of their convertible senior notes. They mention repurchasing $64.5 million of the Notes at a 29% discount. The CFO, Alesia Haas, comments on capital deployment and shareholder value, linking it to their financial discipline and performance in Q1 2023. There's also a mention of forward-looking statements and the usual legal disclaimers about risks and uncertainties.\n",
       "\n",
       "Then there's a second part about CarMax Auto Owner Trust 2019-4 filing some forms with the SEC. It includes details like Commission File Numbers, Central Index Keys, and a signature from Andrew J. McMonigle. This seems like a routine SEC filing related to asset-backed securities, possibly an ABS-EE form. The main points here are the filing of an Asset Data File and an Asset Related Document as required by the SEC.\n",
       "\n",
       "The user wants the main news from the document. The Coinbase section is clearly a significant announcement about their debt repurchase, which is a financial move to create shareholder value. The CarMax part is more of a regulatory filing, which might not be considered \"news\" in the same sense but is part of the document's content. However, since the query is about the main news, the focus should be on Coinbase's announcement. The CarMax filing is probably just additional information included in the context but isn't the primary news. I need to make sure not to mention the CarMax part unless it's relevant. Wait, the user provided both contexts, but the query is about the main news discussed in the document. The first part is a press release, which is news, while the second is a regulatory filing. So the answer should focus on Coinbase's repurchase of notes and the associated details. The CarMax part might be a separate document included in the context but not the main news. Therefore, the main news is Coinbase's repurchase of their convertible senior notes.\n",
       "</think>\n",
       "\n",
       "The main news discussed in the document is Coinbase's announcement of repurchasing $64.5 million of its 0.50% Convertible Senior Notes due 2026 at a 29% discount to par value. The company emphasized this as a strategic capital deployment to enhance shareholder value, reflecting improved financial efficiency and confidence in its business following strong first-quarter performance. The repurchase, expected to close on June 20, 2023, reduces the outstanding notes to $1.373 billion. Additionally, a separate regulatory filing by CarMax Auto Owner Trust 2019-4 regarding asset-backed securities is included, though it is unrelated to the primary Coinbase announcement."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "queries = [\n",
    "    \"What are the main news discussed in the document?\",\n",
    "    \"What are news related to financial sector?\",\n",
    "]\n",
    "\n",
    "for q in queries:\n",
    "    graph_response = query_engine.query(q)\n",
    "    vector_response = vector_query_engine.query(q)\n",
    "\n",
    "    display(Markdown(f\"### Query: {q}\"))\n",
    "    display(Markdown(f\"**GraphRAG:**\\n\\n{graph_response.response}\"))\n",
    "    display(Markdown(f\"**Regular RAG:**\\n\\n{vector_response.response}\"))\n",
    "    display(Markdown(\"---\"))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "llama-index",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
